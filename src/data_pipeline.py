#!/usr/bin/env python3

import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, max as spark_max, desc, asc, to_timestamp, from_unixtime
from pyspark.sql.types import DoubleType
from tabulate import tabulate
import json
from datetime import datetime

# Adicionar o diret√≥rio atual ao path para importar nossa classe
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from quality_check import QualityCheck


class CreditDataPipeline:
    """
    Pipeline de processamento de dados de transa√ß√µes de cr√©dito
    """
    
    def __init__(self):
        self.spark = None
        self.quality_checker = None
        self.raw_df = None
        self.cleaned_df = None
        
    def initialize_spark(self):
        """Inicializar sess√£o Spark"""
        print("üöÄ Inicializando Spark Session...")
        
        self.spark = SparkSession.builder \
            .appName("CreditDataPipeline") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        # Configurar n√≠vel de log
        self.spark.sparkContext.setLogLevel("WARN")
        
        # Inicializar quality checker
        self.quality_checker = QualityCheck(self.spark)
        
        print("‚úÖ Spark Session inicializada!")
        
    def debug_environment(self):
        """Debug do ambiente e volumes"""
        print("\nüîç DEBUG DO AMBIENTE")
        print("=" * 40)
        
        # Verificar diret√≥rio atual
        current_dir = os.getcwd()
        print(f"üìÅ Diret√≥rio atual: {current_dir}")
        
        # Verificar vari√°veis de ambiente
        data_path = os.getenv("DATA_PATH", "/data/df_credit_amostra.csv")
        results_path = os.getenv("RESULTS_PATH", "/results")
        print(f"üìä DATA_PATH: {data_path}")
        print(f"üíæ RESULTS_PATH: {results_path}")
        
        # Verificar se diret√≥rios existem
        dirs_to_check = ["/app", "/data", "/results", "/logs"]
        for dir_path in dirs_to_check:
            if os.path.exists(dir_path):
                print(f"‚úÖ Diret√≥rio existe: {dir_path}")
                try:
                    contents = os.listdir(dir_path)
                    print(f"   üìÇ Conte√∫do ({len(contents)} itens): {contents[:5]}")  # Primeiros 5
                except PermissionError:
                    print(f"   ‚ö†Ô∏è  Sem permiss√£o para listar: {dir_path}")
            else:
                print(f"‚ùå Diret√≥rio N√ÉO existe: {dir_path}")
        
        # Verificar arquivo de dados espec√≠fico
        if os.path.exists(data_path):
            size = os.path.getsize(data_path)
            print(f"‚úÖ Arquivo de dados encontrado: {data_path} ({size} bytes)")
        else:
            print(f"‚ùå Arquivo de dados N√ÉO encontrado: {data_path}")
            
        # Verificar permiss√µes do diret√≥rio results
        try:
            test_file = os.path.join(results_path, "test_write.txt")
            with open(test_file, 'w') as f:
                f.write("teste")
            os.remove(test_file)
            print(f"‚úÖ Diret√≥rio results √© grav√°vel: {results_path}")
        except Exception as e:
            print(f"‚ùå Erro ao testar grava√ß√£o em results: {e}")
        
    def load_data(self, file_path: str):
        """Carregar dados do arquivo CSV"""
        print(f"\nüìÇ Carregando dados de: {file_path}")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")
        
        self.raw_df = self.spark.read.option("header", "true").option("inferSchema", "true").csv(file_path)
        
        print(f"üìä Dados carregados: {self.raw_df.count()} registros, {len(self.raw_df.columns)} colunas")
        
        # Mostrar schema
        print("\n Schema dos dados:")
        self.raw_df.printSchema()
        
        return self.raw_df
    
    def clean_data(self):
        """Limpeza e transforma√ß√£o dos dados"""
        print("\n Iniciando limpeza dos dados...")
        
        if self.raw_df is None:
            raise ValueError("Dados n√£o carregados. Execute load_data() primeiro.")
        
        # Fazer uma c√≥pia para limpeza
        df = self.raw_df
        
        print("üìù Aplicando transforma√ß√µes:")
        
        # 1. Converter risk_score para double (tratar 'none' como null)
        print("   - Convertendo risk_score para double")
        df = df.withColumn("risk_score_clean", 
                          col("risk_score").cast("string"))
        
        # Substituir 'none' por null e converter para double
        from pyspark.sql.functions import when
        df = df.withColumn("risk_score", 
                          when(col("risk_score_clean") == "none", None)
                          .otherwise(col("risk_score_clean").cast(DoubleType())))
        df = df.drop("risk_score_clean")
        
        # 2. Converter timestamp para formato de data leg√≠vel
        print("   - Convertendo timestamp para formato de data")
        df = df.withColumn("timestamp_readable", from_unixtime(col("timestamp"), "yyyy-MM-dd HH:mm:ss"))
        
        # 3. Remover registros com valores cr√≠ticos nulos (exceto risk_score)
        print("   - Removendo registros com valores cr√≠ticos nulos")
        initial_count = df.count()
        
        # Colunas cr√≠ticas que n√£o podem ser nulas (removemos risk_score da lista)
        critical_columns = ["timestamp", "receiving_address", "amount", "transaction_type", "location_region"]
        
        for column in critical_columns:
            df = df.filter(col(column).isNotNull())
        
        final_count = df.count()
        removed_count = initial_count - final_count
        
        if removed_count > 0:
            print(f"   ‚ö†Ô∏è  Removidos {removed_count} registros com valores nulos em colunas cr√≠ticas")
        
        # 4. Filtrar valores de amount negativos ou zero (se existirem)
        print("   - Filtrando amounts inv√°lidos")
        df = df.filter(col("amount") > 0)
        
        # 5. Padronizar strings
        print("   - Padronizando strings")
        string_columns = ["transaction_type", "location_region", "purchase_pattern", "age_group", "anomaly"]
        for column in string_columns:
            if column in df.columns:
                df = df.withColumn(column, col(column).cast("string"))
        
        self.cleaned_df = df
        
        print(f"‚úÖ Limpeza conclu√≠da! Registros finais: {self.cleaned_df.count()}")
        
        return self.cleaned_df
    
    def create_location_risk_analysis(self):
        """
        Criar tabela-resultado: location_region por m√©dia de risk_score (ordem decrescente)
        """
        print("\nüìç Criando an√°lise de risco por localiza√ß√£o...")
        
        if self.cleaned_df is None:
            raise ValueError("Dados limpos n√£o dispon√≠veis. Execute clean_data() primeiro.")
        
        # Filtrar apenas registros com risk_score v√°lido para esta an√°lise
        valid_risk_df = self.cleaned_df.filter(col("risk_score").isNotNull())
        
        # Calcular m√©dia de risk_score por location_region
        location_risk_df = valid_risk_df.groupBy("location_region") \
            .agg(
                avg("risk_score").alias("avg_risk_score"),
                col("location_region")
            ) \
            .select("location_region", "avg_risk_score") \
            .orderBy(desc("avg_risk_score"))
        
        print("üìä Resultado - Location Regions por M√©dia de Risk Score:")
        
        # Converter para Pandas para visualiza√ß√£o
        result_pandas = location_risk_df.toPandas()
        result_pandas["avg_risk_score"] = result_pandas["avg_risk_score"].round(2)
        
        print(tabulate(result_pandas, headers=['Location Region', 'Avg Risk Score'], 
                      tablefmt='grid', showindex=False))
        
        # Salvar resultado - M√öLTIPLOS FORMATOS
        results_path = os.getenv("RESULTS_PATH", "/results")
        
        # CSV
        csv_path = os.path.join(results_path, "location_risk_analysis.csv")
        result_pandas.to_csv(csv_path, index=False)
        print(f"üíæ Resultado salvo em CSV: {csv_path}")
        
        # JSON
        json_path = os.path.join(results_path, "location_risk_analysis.json")
        result_pandas.to_json(json_path, orient='records', indent=2)
        print(f"üíæ Resultado salvo em JSON: {json_path}")
        
        # Parquet via Spark (para compatibilidade)
        try:
            parquet_path = os.path.join(results_path, "location_risk_analysis_parquet")
            location_risk_df.coalesce(1).write.mode("overwrite").parquet(parquet_path)
            print(f"üíæ Resultado salvo em Parquet: {parquet_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Aviso: N√£o foi poss√≠vel salvar Parquet: {e}")
        
        return location_risk_df
    
    def create_top_sale_addresses_analysis(self):
        """
        Criar tabela-resultado: 3 receiving_address com maior amount 
        considerando apenas a transa√ß√£o mais recente de "sale" de cada receiving_address
        """
        print("\nüí∞ Criando an√°lise dos top 3 receiving addresses para vendas...")
        
        if self.cleaned_df is None:
            raise ValueError("Dados limpos n√£o dispon√≠veis. Execute clean_data() primeiro.")
        
        # Filtrar apenas transa√ß√µes do tipo "sale"
        sales_df = self.cleaned_df.filter(col("transaction_type") == "sale")
        
        print(f"   üìã Total de transa√ß√µes de venda: {sales_df.count()}")
        
        if sales_df.count() == 0:
            print("   ‚ö†Ô∏è  Nenhuma transa√ß√£o de venda encontrada!")
            return None
        
        # Para cada receiving_address, pegar apenas a transa√ß√£o mais recente
        from pyspark.sql.window import Window
        from pyspark.sql.functions import row_number
        
        # Criar window function particionando por receiving_address e ordenando por timestamp desc
        window_spec = Window.partitionBy("receiving_address").orderBy(desc("timestamp"))
        
        # Adicionar row_number e filtrar apenas a linha 1 (mais recente)
        latest_sales_df = sales_df.withColumn("row_num", row_number().over(window_spec)) \
            .filter(col("row_num") == 1) \
            .drop("row_num")
        
        # Ordenar por amount decrescente e pegar os top 3
        top_3_sales = latest_sales_df.orderBy(desc("amount")).limit(3)
        
        # Selecionar apenas as colunas necess√°rias
        result_df = top_3_sales.select("receiving_address", "amount", "timestamp", "timestamp_readable")
        
        print("üèÜ Top 3 Receiving Addresses (√∫ltima transa√ß√£o de venda):")
        
        # Converter para Pandas para visualiza√ß√£o
        result_pandas = result_df.toPandas()
        
        print(tabulate(result_pandas[["receiving_address", "amount", "timestamp_readable"]], 
                      headers=['Receiving Address', 'Amount', 'Timestamp'], 
                      tablefmt='grid', showindex=False))
        
        # Salvar resultado - M√öLTIPLOS FORMATOS
        results_path = os.getenv("RESULTS_PATH", "/results")
        
        # CSV
        csv_path = os.path.join(results_path, "top_sale_addresses.csv")
        result_pandas.to_csv(csv_path, index=False)
        print(f"üíæ Resultado salvo em CSV: {csv_path}")
        
        # JSON
        json_path = os.path.join(results_path, "top_sale_addresses.json")
        result_pandas.to_json(json_path, orient='records', indent=2)
        print(f"üíæ Resultado salvo em JSON: {json_path}")
        
        # Parquet via Spark
        try:
            parquet_path = os.path.join(results_path, "top_sale_addresses_parquet")
            result_df.coalesce(1).write.mode("overwrite").parquet(parquet_path)
            print(f"üíæ Resultado salvo em Parquet: {parquet_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Aviso: N√£o foi poss√≠vel salvar Parquet: {e}")
        
        return result_df
    
    def save_quality_reports(self):
        """Salvar relat√≥rios de qualidade"""
        results_path = os.getenv("RESULTS_PATH", "/results")
        
        # Salvar relat√≥rio JSON
        json_path = os.path.join(results_path, "quality_report.json")
        self.quality_checker.save_report_to_file(json_path)
        
        # Criar relat√≥rio resumido em TXT
        txt_path = os.path.join(results_path, "quality_summary.txt")
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("RELAT√ìRIO DE QUALIDADE DE DADOS\n")
            f.write("=" * 50 + "\n\n")
            
            if 'cleaned_data' in self.quality_checker.quality_report:
                report = self.quality_checker.quality_report['cleaned_data']
                f.write(f"Timestamp: {report['timestamp']}\n")
                f.write(f"Total de registros: {report['basic_metrics']['total_records']}\n")
                f.write(f"Total de colunas: {report['basic_metrics']['total_columns']}\n")
                f.write(f"Score de qualidade: {report['quality_score']}/100\n\n")
                
                f.write(f"Colunas com nulos: {report['null_analysis']['columns_with_nulls']}\n")
                f.write(f"Registros duplicados: {report['duplicate_analysis']['duplicate_count']}\n")
                f.write(f"Anomalias detectadas: {len(report['anomalies'])}\n")
        
        print(f"üíæ Relat√≥rio de qualidade salvo: {txt_path}")
    
    def run_pipeline(self, file_path: str = "/data/df_credit_amostra.csv"):
        """Executar pipeline completo"""
        try:
            print("üéØ INICIANDO PIPELINE DE DADOS DE CR√âDITO v2.0")
            print("="*60)
            
            # Debug do ambiente
            self.debug_environment()
            
            # Inicializar Spark
            self.initialize_spark()
            
            # Carregar dados
            self.load_data(file_path)
            
            # Verificar qualidade dos dados brutos
            print("\nüîç VERIFICA√á√ÉO DE QUALIDADE - DADOS BRUTOS")
            self.quality_checker.check_data_quality(self.raw_df, "raw_data")
            
            # Limpar dados
            self.clean_data()
            
            # Verificar qualidade dos dados limpos
            print("\nüîç VERIFICA√á√ÉO DE QUALIDADE - DADOS LIMPOS")
            self.quality_checker.check_data_quality(self.cleaned_df, "cleaned_data")
            
            # Executar an√°lises
            print("\nüìä EXECUTANDO AN√ÅLISES")
            print("-" * 40)
            
            self.create_location_risk_analysis()
            self.create_top_sale_addresses_analysis()
            
            # Salvar relat√≥rios finais
            self.save_quality_reports()
            
            print("\nüéâ PIPELINE EXECUTADO COM SUCESSO!")
            
            # Verificar arquivos salvos
            results_path = os.getenv("RESULTS_PATH", "/results")
            if os.path.exists(results_path):
                files = os.listdir(results_path)
                print(f"üìÅ Arquivos salvos em {results_path}:")
                for file in sorted(files):
                    file_path_full = os.path.join(results_path, file)
                    if os.path.isfile(file_path_full):
                        size = os.path.getsize(file_path_full)
                        print(f"   üìÑ {file} ({size} bytes)")
                    else:
                        print(f"   üìÅ {file}/ (diret√≥rio)")
            
        except Exception as e:
            print(f"‚ùå Erro na execu√ß√£o do pipeline: {str(e)}")
            import traceback
            traceback.print_exc()
            raise e
        
        finally:
            if self.spark:
                print("\nüîö Encerrando Spark Session...")
                self.spark.stop()


def main():
    """Fun√ß√£o principal"""
    pipeline = CreditDataPipeline()
    
    # Verificar se arquivo existe
    file_path = "/data/df_credit_amostra.csv"
    if not os.path.exists(file_path):
        # Tentar path local para desenvolvimento
        file_path = "df_credit_amostra.csv"
        if not os.path.exists(file_path):
            print("‚ùå Arquivo de dados n√£o encontrado!")
            print("   Verificar se o arquivo est√° em /data/df_credit_amostra.csv")
            return
    
    pipeline.run_pipeline(file_path)


if __name__ == "__main__":
    main()
