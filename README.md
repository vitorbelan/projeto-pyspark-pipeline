# ğŸ¯ Pipeline de Dados de CrÃ©dito com PySpark

Este projeto implementa uma pipeline completa de processamento de dados de transaÃ§Ãµes de crÃ©dito usando **PySpark**, **Docker**, e **Apache Airflow** para orquestraÃ§Ã£o, com monitoramento automÃ¡tico de qualidade de dados.

## ğŸ“‹ VisÃ£o Geral

Aqui trouxe um projeto estruturadado onde a ideia e conseguir rodar usando docker mas que ele traga uma previa dos resultados no proprio terminal.

pontos basicos e rapidos
necessario colocar o arquivo de ingestao dentro da pasta data e ter docker rodando. depois so seguir a estruturacao do README.md
com o docker instalado rodar com comando ./run-complete.sh all

A pipeline processa dados de transaÃ§Ãµes financeiras e realiza:
- âœ… **ImportaÃ§Ã£o** e limpeza de dados CSV
- ğŸ“Š **AnÃ¡lise de risco** por regiÃ£o geogrÃ¡fica  
- ğŸ’° **IdentificaÃ§Ã£o** dos top 3 receiving addresses por valor
- ğŸ” **Monitoramento automÃ¡tico** de qualidade de dados
- ğŸ“ˆ **MÃ©tricas** e alertas de conformidade

## ğŸ—ï¸ Arquitetura

```
â”œâ”€â”€ ğŸ³ ContainerizaÃ§Ã£o (Docker + Docker Compose)
â”œâ”€â”€ âš¡ Processamento (Apache Spark/PySpark)
â”œâ”€â”€ âœˆï¸ OrquestraÃ§Ã£o (Apache Airflow)
â”œâ”€â”€ ğŸ“Š Monitoramento (Spark UI + Quality Checks)
â””â”€â”€ ğŸ” Data Quality (Classe personalizada QualityCheck)
```

## ğŸ“ Estrutura do Projeto

```
projeto-pyspark-pipeline/
â”œâ”€â”€ ğŸ³ docker/
â”‚   â”œâ”€â”€ Dockerfile                    # Imagem customizada com PySpark
â”‚   â””â”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ ğŸ“„ src/
â”‚   â”œâ”€â”€ quality_check.py              # Classe para Data Quality
â”‚   â”œâ”€â”€ data_pipeline.py              # Pipeline principal PySpark
â”‚   â””â”€â”€ config.py                     # ConfiguraÃ§Ãµes
â”œâ”€â”€ ğŸ“Š data/
â”‚   â””â”€â”€ df_credit_amostra.csv          # Dados de entrada
â”œâ”€â”€ âœˆï¸ airflow/
â”‚   â”œâ”€â”€ dags/credit_pipeline_dag.py   # DAG do Airflow
â”‚   â””â”€â”€ docker-compose-airflow.yml    # Setup do Airflow
â”œâ”€â”€ ğŸ”§ requirements.txt               # DependÃªncias Python
â”œâ”€â”€ ğŸš€ run.sh                        # Script de execuÃ§Ã£o
â””â”€â”€ ğŸ“š README.md                     # Esta documentaÃ§Ã£o
```

## ğŸš€ Como Executar

### 1ï¸âƒ£ **PreparaÃ§Ã£o dos Dados**

```bash
# 1. Clone ou baixe o projeto
# 2. Coloque o arquivo CSV na pasta data/
cp df_credit_amostra.csv data/
```

### 2ï¸âƒ£ **ExecuÃ§Ã£o Simples (Pipeline)**

```bash
# Dar permissÃ£o de execuÃ§Ã£o ao script
chmod +x run.sh

# Executar apenas o pipeline principal
./run.sh pipeline
```

### 3ï¸âƒ£ **ExecuÃ§Ã£o com Jupyter (Desenvolvimento)**

```bash
# Executar pipeline + Jupyter Lab
./run.sh jupyter

# Acessar: http://localhost:8888
```

### 4ï¸âƒ£ **ExecuÃ§Ã£o com Monitoramento**

```bash
# Executar com Spark History Server
./run.sh monitoring

# Acessar Spark UI: http://localhost:18080
```

### 5ï¸âƒ£ **ExecuÃ§Ã£o com Airflow (OrquestraÃ§Ã£o)**

```bash
# Configurar e executar Airflow
./run.sh airflow

# Acessar: http://localhost:8080
# UsuÃ¡rio: admin | Senha: admin123
```

### 6ï¸âƒ£ **ExecuÃ§Ã£o Completa**

```bash
# Executar tudo junto
./run.sh all

# ServiÃ§os disponÃ­veis:
# - Pipeline: ExecuÃ§Ã£o Ãºnica
# - Jupyter: http://localhost:8888  
# - Monitoring: http://localhost:18080
```

## ğŸ“Š Resultados Gerados

A pipeline gera os seguintes resultados na pasta `results/`:

### 1. **AnÃ¡lise de Risco por RegiÃ£o**
```
Location Regions por MÃ©dia de Risk Score (ordem decrescente):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Location Region â”‚ Avg Risk Score â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Africa          â”‚          62.50 â”‚
â”‚ Asia            â”‚          31.25 â”‚
â”‚ South America   â”‚          30.88 â”‚
â”‚ Europe          â”‚          18.75 â”‚
â”‚ North America   â”‚          12.50 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Top 3 Receiving Addresses (Vendas)**
```
Top 3 Receiving Addresses (Ãºltima transaÃ§Ã£o de venda):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Receiving Address                        â”‚  Amount â”‚ Timestamp           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0x6fdc047c2391615b3facd79b4588c7e9106e49f2 â”‚ 66002.0 â”‚ 2021-01-21 19:01:35 â”‚
â”‚ 0x4d220aa8bf8a866b1c8da34c900e8f783e5c98d7 â”‚ 35623.0 â”‚ 2020-04-19 10:15:22 â”‚
â”‚ 0x1f2f48e9c4b79d2e2a1a3d4e5f6a7b8c9d0e1f2a â”‚  1500.0 â”‚ 2021-11-15 08:30:45 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. **RelatÃ³rio de Qualidade de Dados**
```json
{
  "cleaned_data": {
    "timestamp": "2024-12-08T15:30:42.123456",
    "basic_metrics": {
      "total_records": 17,
      "total_columns": 13
    },
    "quality_score": 95.5,
    "null_analysis": {
      "columns_with_nulls": 0
    },
    "anomalies": []
  }
}
```

## ğŸ” Data Quality - Classe QualityCheck

A classe `QualityCheck` implementa monitoramento automÃ¡tico com:

### ğŸ“Š **MÃ©tricas Calculadas:**
- âœ… Total de registros e colunas
- ğŸ” AnÃ¡lise de valores nulos (por coluna)
- ğŸ”„ DetecÃ§Ã£o de registros duplicados
- ğŸ“ˆ EstatÃ­sticas descritivas (mÃ©dia, desvio, min/max)
- âš ï¸ DetecÃ§Ã£o de anomalias automatizada

### ğŸ“ˆ **Score de Qualidade (0-100):**
- **100**: Dados perfeitos
- **80-99**: Qualidade alta
- **60-79**: Qualidade mÃ©dia (alertas)  
- **<60**: Qualidade baixa (aÃ§Ã£o necessÃ¡ria)

### ğŸš¨ **Alertas AutomÃ¡ticos:**
- Valores negativos em `amount`
- Timestamps muito antigos/futuros
- Percentual alto de nulos
- Registros duplicados em excesso

## âœˆï¸ OrquestraÃ§Ã£o com Airflow

O DAG `credit_data_pipeline` implementa:

### ğŸ“‹ **Tasks:**
1. `check_input_file` - Verificar arquivo de entrada
2. `run_credit_pipeline` - Executar pipeline PySpark
3. `validate_results` - Validar resultados gerados
4. `send_quality_alerts` - Enviar alertas de qualidade
5. `generate_execution_report` - Gerar relatÃ³rio final
6. `cleanup_temp_files` - Limpeza de arquivos temporÃ¡rios

### â° **Agendamento:**
- **Schedule:** A cada 6 horas
- **Retry:** 2 tentativas com 5 min de intervalo
- **Alertas:** Email em caso de falha

### ğŸ”§ **ConfiguraÃ§Ã£o:**
```python
# Editar em airflow/dags/credit_pipeline_dag.py
default_args = {
    'email': ['seu-email@empresa.com'],  # â† Configurar email
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}
```

## ğŸ³ Comandos Docker

### **Build Manual**
```bash
docker build -t credit-data-pipeline:latest .
```

### **Executar Container Individual**
```bash
docker run --rm \
  -v $(pwd)/data:/data:ro \
  -v $(pwd)/results:/results \
  -v $(pwd)/src:/app/src \
  credit-data-pipeline:latest
```

### **Docker Compose - Pipeline**
```bash
docker-compose up --build pyspark-pipeline
```

### **Docker Compose - Com Jupyter**
```bash
docker-compose --profile jupyter up -d
```

## ğŸ“Š Monitoramento e Logs

### **Logs da Pipeline**
```bash
# Ver logs em tempo real
./run.sh logs

# Ou diretamente
docker-compose logs -f pyspark-pipeline
```

### **Spark UI (History Server)**
```bash
# Iniciar monitoramento
./run.sh monitoring

# Acessar: http://localhost:18080
```

### **Jupyter Lab (Desenvolvimento)**
```bash
# AnÃ¡lise interativa
./run.sh jupyter

# Acessar: http://localhost:8888
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### **Ajustar Recursos Spark**
```yaml
# Em docker-compose.yml
environment:
  - SPARK_DRIVER_MEMORY=4g      # â† Ajustar conforme necessÃ¡rio
  - SPARK_EXECUTOR_MEMORY=4g    # â† Ajustar conforme necessÃ¡rio
  - SPARK_EXECUTOR_CORES=2      # â† Ajustar conforme necessÃ¡rio
```

### **Configurar Quality Thresholds**
```python
# Em src/config.py
QUALITY_THRESHOLDS = {
    "min_quality_score": 70.0,        # â† Score mÃ­nimo aceitÃ¡vel
    "max_null_percentage": 10.0,      # â† % mÃ¡ximo de nulos
    "max_duplicate_percentage": 5.0   # â† % mÃ¡ximo de duplicados
}
```

## ğŸš€ Scripts de UtilitÃ¡rio

### **Parar Todos os ServiÃ§os**
```bash
./run.sh stop
```

### **Limpar Ambiente Completo**
```bash
./run.sh clean
```

### **Ajuda Completa**
```bash
./run.sh help
```

## ğŸ“‹ DependÃªncias

### **Sistema:**
- ğŸ³ Docker 20.10+
- ğŸ™ Docker Compose 2.0+
- ğŸ’» 4GB+ RAM recomendado
- ğŸ’¾ 2GB+ espaÃ§o em disco

### **Python (instaladas automaticamente):**
- `pyspark==3.4.1`
- `pandas>=2.0.0`
- `tabulate>=0.9.0`
- Ver `requirements.txt` completo

## â— ResoluÃ§Ã£o de Problemas

### **Problema: "Arquivo nÃ£o encontrado"**
```bash
# Verificar se o arquivo estÃ¡ no local correto
ls -la data/df_credit_amostra.csv

# Deve retornar o arquivo
```

### **Problema: "Docker nÃ£o estÃ¡ rodando"**
```bash
# Iniciar Docker Desktop
# Ou no Linux:
sudo systemctl start docker
```

### **Problema: "Porta jÃ¡ estÃ¡ em uso"**
```bash
# Ver o que estÃ¡ usando a porta
lsof -i :8080

# Parar serviÃ§os conflitantes
./run.sh stop
```

### **Problema: "PermissÃ£o negada no run.sh"**
```bash
chmod +x run.sh
```

### **Problema: "MemÃ³ria insuficiente"**
```bash
# Reduzir recursos no docker-compose.yml
environment:
  - SPARK_DRIVER_MEMORY=1g
  - SPARK_EXECUTOR_MEMORY=1g
```

## ğŸ”® PrÃ³ximos Passos

### **PossÃ­veis Melhorias:**
- ğŸ”” IntegraÃ§Ã£o com Slack/Teams para alertas
- ğŸ“ˆ Dashboard em tempo real (Grafana)
- ğŸ—ƒï¸ IntegraÃ§Ã£o com data warehouse (BigQuery/Snowflake)
- ğŸ”„ CI/CD pipeline com GitHub Actions
- ğŸ§ª Testes automatizados (pytest)
- ğŸ“Š MÃ©tricas avanÃ§adas de data drift

### **Escalabilidade:**
- â˜¸ï¸ MigraÃ§Ã£o para Kubernetes
- ğŸ“¡ Streaming com Kafka + Spark Streaming
- ğŸŒ Deploy em cloud (AWS EMR, Databricks)

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork do projeto
2. Criar branch para feature (`git checkout -b feature/AmazingFeature`)
3. Commit das mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para branch (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido como soluÃ§Ã£o completa de pipeline de dados com PySpark, Docker e Airflow.

---

**ğŸ¯ Ready to process some data? Execute `./run.sh pipeline` and let's go!**
